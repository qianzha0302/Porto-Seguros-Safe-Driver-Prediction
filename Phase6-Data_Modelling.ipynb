{"cells":[{"cell_type":"code","metadata":{"cell_id":"5e78fa4c8f084412ad4b9ecce9c2ccb6","deepnote_cell_type":"code"},"source":"# import packages\n\n# data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta, datetime\n\n\nimport re\n\n# data visualization\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import Bar, Layout\nfrom plotly import offline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (20, 10)\n\nplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\nplt.rcParams['axes.unicode_minus'] = False #用来正常显示负号\n\n# change text color\nimport colorama\nfrom colorama import Fore, Style\n\n# IPython\nfrom IPython.display import IFrame\n\nfrom sklearn.feature_selection import mutual_info_classif\n\n%matplotlib inline","block_group":"f649c7df486c42deb2480fc5a1d9beef","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"549a64d3a73a43d3a7d02fb89f9a7891","deepnote_cell_type":"markdown"},"source":"## Scoring Metrics","block_group":"c79b821f97c94a30bbd01e8976e47967"},{"cell_type":"markdown","metadata":{"cell_id":"4a35eb93c30646d99bf70ca3ec178076","deepnote_cell_type":"markdown"},"source":"<div class=\"alert alert-block alert-success\"><b>Step 1</b>: \n    \n\n1. normalized gini coefficent\n    \n</div>","block_group":"3b3cd2b99975431d94c0cf654eaa1ffa"},{"cell_type":"code","metadata":{"cell_id":"c4a92e59448a4ac282bba2fa52315b8e","deepnote_cell_type":"code"},"source":"def eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = -eval_gini(labels, preds)\n    return [('gini', gini_score)]","block_group":"d201c00b66ae476181e3d94eb3af1948","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"5ca63084ae934bb6b07b29c0405df63c","deepnote_cell_type":"markdown"},"source":"## Underfitting AND Overfitting\n\n### Underfitting\nA statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n\n### Overfitting\nA statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. ","block_group":"92effee4c2b3422e998ab84368c56728"},{"cell_type":"markdown","metadata":{"cell_id":"b58bbf22680d4abb8751d9ee9500c78c","deepnote_cell_type":"markdown"},"source":"**Reasons for Underfitting:**\n1.The model is too simple, So it may be not capable to represent the complexities in the data.\n2.The input features which is used to train the model is not the adequate representations of underlying factors 3.influencing the target variable.\n4.The size of the training dataset used is not enough.\n5.Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n6.Features are not scaled.\n\n**Techniques to Reduce Underfitting**\n1.Increase model complexity.\n2.Increase the number of features, performing feature engineering.\n3.Remove noise from the data.\n4.Increase the number of epochs or increase the duration of training to get better results.\n\n**Reasons for Overfitting:**\n1.High variance and low bias.\n2.The model is too complex.\n3.The size of the training data.\n\n**Techniques to Reduce Overfitting:**\n1.Increase training data.\n2.Reduce model complexity.\n3.Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins 4.to increase stop training).\n5.Ridge Regularization and Lasso Regularization.\n6.Use dropout for neural networks to tackle overfitting.","block_group":"85468475b6204007a5972e05c9c97b67"},{"cell_type":"markdown","metadata":{"cell_id":"04c96747b7334bd4866d03d4266e8a5f","deepnote_cell_type":"markdown"},"source":"## Cross validation","block_group":"a4b27c37ac4c410c8ad6d9b53c5c2b58"},{"cell_type":"markdown","metadata":{"cell_id":"be6e36c549c5479482e5dd3c56f2f7ea","deepnote_cell_type":"markdown"},"source":"Cross-validation is a statistical method used to evaluate the performance of machine learning models. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds.\n\n### When to use Cross Validation\n\nCross-validation is used when data is not sufficient. For example, if the data sample size is less than 10,000, we will use cross-validation to train the optimization selection model. If there are more than 10,000 samples, we usually randomly divide the data into three parts, one is the training set (Training Set), one is the validation set (Validation Set), and the last part is the test set (Test Set). \n\nWe Use the training set to train the model, and the validation set to evaluate the quality of model predictions and select the model and its corresponding parameters. Use the final model on the test set to finally decide which model to use and the corresponding parameters.","block_group":"d4d205f2028a40f6ab667b44f32e8545"},{"cell_type":"markdown","metadata":{"cell_id":"73ff6cc586644f6192cb5f8e4e44cee0","deepnote_cell_type":"markdown"},"source":"### What are the uses of cross-validation?\n\n1.Model selection, also known as hyperparameter selection.\n2.Model evaluation, using cross-validation to assess the performance of a model.\n\n### What are the methods of cross-validation?\n\n**The first one is Simple Cross-Validation:**\nThe term 'simple' is relative to other cross-validation methods. Initially, the sample data is randomly divided into two parts (e.g., 70% training set and 30% test set). Then, the model is trained on the training set and validated on the test set. After that, the samples are shuffled, and new training and test sets are chosen to continue training and checking the model. Finally, a loss function is used to evaluate the best model and parameters.\n\n**The second one is S-Fold Cross-Validation:**\nUnlike the first method, S-Fold Cross-Validation randomly divides the sample data into S parts. In each round, S-1 parts are chosen as the training set, and the remaining 1 part is used as the test set. After this round, a new random selection of S-1 parts is used for training the data. After several rounds (fewer than S), a loss function is used to evaluate the best model and parameters.\n\n**The third one is Leave-One-Out Cross-Validation:**\nIt is a special case of the second method, where S is equal to the number of samples, N. In this case, for N samples, in each round, N-1 samples are used for training, leaving one sample for validating the quality of the model's predictions. This method is mainly used when the sample size is very small, such as in cases where N is less than 50, for moderately sized problems. I generally use Leave-One-Out Cross-Validation in such cases.\"\n\n    \n### Difference between the validation set and the test set:\n\n1.The validation set is used during the training process to assess the model's training performance and determine suitable hyperparameters.\n2.The test set is used after training is completed to evaluate the model's generalization ability.\"","block_group":"cad94ee3d8294bd2ba36efecc88019bd"},{"cell_type":"code","metadata":{"cell_id":"6f5c8ba2e50c4ad0be22c14d657b3063","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import KFold\n\nK = 10\nkf = KFold(n_splits = K, random_state = 1, shuffle = True)\nnp.random.seed(1996)","block_group":"f1946dbe0a2b424b8d23476119313d68","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"19f05507eac34d8cb060faf9b50574c4","deepnote_cell_type":"markdown"},"source":"## logistic regression\n\n\n![](https://saedsayad.com/images/LogReg_1.png)\n\nLogistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables. The independent variables can be nominal, ordinal, or of interval type.\n\nThe name “logistic regression” is derived from the concept of the logistic function that it uses. The logistic function is also known as the sigmoid function. The value of this logistic function lies between zero and one.\n\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)","block_group":"8029d54767fe4e1584b020a4daf62b65"},{"cell_type":"markdown","metadata":{"cell_id":"8dbdd7517488416fbe889ed483048f4d","deepnote_cell_type":"markdown"},"source":"参考：https://zg104.github.io/Logistic_regression","block_group":"081cbe555f344136a7be421a888b8b56"},{"cell_type":"markdown","metadata":{"cell_id":"8b42224b8b4e4056bc3d3c583584e515","deepnote_cell_type":"markdown"},"source":"**Why we use logistic regression, not linear regression? What are the disadvantages of linear regression for classification?**\n\n- Linear regression can give us the values which are not between 0 and 1.\n\n- Also, linear regression is sensitive to the outliers. However, the sigmoid function restrict the values between 0 and 1, which can be interpreted as the conditional probability of assigning the data to the particular class given the data parametrized by theta.\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/linear-regression-vs-logistic-regression.png)\n\n**What type of datasets is most suited for logistic regression?**\n\n- Logistic regression likes overlapping data, instead of well separated data.\n- Linear Discriminent Analysis will perform better for well separated data since the decision boundary is linear.\n\n**Can you explain or interpret the hypothesis output of logistic regression?**\n\n- We try to set a threshold to determine which class each data point should be assigned based on the conditional probability (I have clarified in Q1) derived from the sigmoid function.\n- Typically, we set the threshold to be 0.5. However, it can be adjusted between 0 and 1 for personal specification, such as restriction on TPR (True Positive Rate).\n\n\n**Why we define the sigmoid function, create a new version of cost function, and applied MLE to derive logistic regression?**\n\n- Sigmoid function helps transform the linear esitimation into non-linear one.\n\n- If we use the mean squared error as the cost function the same as linear regression, it is impossible to find the derivatives of the cost function with respect to theta, since the sigmoid function will make the cost function non-convex. So, we have to use gradient descent to minimize the cost function instead of computing the gradient by hand.\n\n- You might wonder why the cost function of logistic regression is like this! That is beacuse we applied the MLE to maximize the probability to make the model the most plausible for all data points. You always minimize the loss function, which is just the negative form of the loglikelihood after MLE.\n\n**How to deal with overfitting?**\n\n- It can be pretty easy for every machine learning method to be overfitting. It is not a big deal!\n\n- A regularization term is added to the cost function where the first part is loss function, and the second is the penalty term.\n\n![](https://miro.medium.com/max/3232/1*vwhvjVQiEgLcssUPX6vxig.png)\n\n**What are the disadvantage of logistic regression?**\n\n- You should use k-fold cross validation to determine the highest polynomial of the features if the decision boundary is non-linear. It can be easy for this to overfit.\n\n- Logistic regression is unstable when dealing with well separated datasets.\n\n- Logistic regression requires relatively large datasets for training.\n\n- Logistic regression is not that popular for multiclassification problems. Sigmoid function should be ungraded to Softmax function(You may hear about it if you know about Neural Networks).","block_group":"6ccc0d1af6994ccb816dcfd9516d2340"},{"cell_type":"markdown","metadata":{"cell_id":"5684a4ba1f63487ba5eb30233738d334","deepnote_cell_type":"markdown"},"source":"<span id=\"5\"></span>In order to get results between 0 and 1, a function, which is called **sigmoid**, is used to transform our hypothesis function. It is defined as\n$$ $$\n$$h_{\\theta}(x) = g(\\theta^{T} x)$$ \n$$ $$\nwhere $h_{\\theta}(x)$ is the hypothesis function, $x$ is a single record and \n$$ $$\n$$g(z)=\\dfrac{1}{1+e^{-z}}$$\n$$ $$\nBy using $g(\\theta^{T} x)$, we obtain the probablity and if $h_{\\theta}(x) \\geq 0.5$, we get $y=1$; if $h_{\\theta}(x) < 0.5$, we get $y=0$. Further, when $z \\geq 0$, $g(z) \\geq 0.5$ is another detail. Thus, if the $\\theta^{T} x \\geq 0$, then $y=1$.\n \nBy the definition, I defined the below ***sigmoid*** function.<span id=\"5\"></span>\n\nWe can't use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. That's why we need to define a different cost function for logistic regression. It is simply defined as\n$$ $$\n$$J(\\theta) = \\dfrac{1}{m} \\sum^{m}_{i=1}Cost(h_{\\theta}(x^{(i)}), y^{(i)})$$ \n$$ $$\nwhere \n$$ $$\n$$Cost(h_{\\theta}(x^{(i)}), y^{(i)})=-y^{(i)} \\; log(h_{\\theta}(x^{(i)}))-(1-y^{(i)}) \\; log(1-h_{\\theta}(x^{(i)}))$$\n$$ $$\nAs the sanity check, $J(\\theta)$ can be plotted or printed as a function of the number of iterations to be sure that $J(\\theta)$ is **decreasing on every iteration**, which shows that it is converging correctly. At this point, choice of $\\alpha$ is important. If we select a high or small $\\alpha$ value, we might have problem about the converging.<span id=\"6\"></span>\n\n","block_group":"1a951afb77764f4ca88d2f72fd340cea"},{"cell_type":"code","metadata":{"scrolled":true,"cell_id":"ad7cff449ab841e188dafa108938ce7b","deepnote_cell_type":"code"},"source":"# prepare the data\n\n# 1. 写出Sigmoid function\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n    \n# 2. loss function \ndef loss(h, y):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()","block_group":"6197c03b5a33401bb86d168a0076a524","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"72dec43ef7fd4a418cc4e873f1c0ca76","deepnote_cell_type":"markdown"},"source":"## XGBoost\n\nXGBoost is a supervised machine learning method for classification and regression and is used by the Train Using AutoML tool. XGBoost is short for extreme gradient boosting. This method is based on decision trees and improves on other methods such as random forest and gradient boost. \n\nTo fit a training dataset using XGBoost, an initial prediction is made. Residuals are computed based on the predicted value and the observed values. A decision tree is created with the residuals using a similarity score for residuals. The similarity of the data in a leaf is calculated, as well as the gain in similarity in the subsequent split. The gains are compared to determine a feature and a threshold for a node. The output value for each leaf is also calculated using the residuals. For classification, the values are typically calculated using the log of odds and probabilities. The output of the tree becomes the new residual for the dataset, which is used to construct another tree. This process is repeated until the residuals stop reducing or for a specified number of times. Each subsequent tree learns from the previous trees and is not assigned equal weight, unlike how Random Forest works.","block_group":"66743dc3545a4d47a387c26eed6e7154"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d70f755b5d10445da6d23d885164f902","deepnote_cell_type":"text-cell-h3"},"source":"### Advantages of XGBoost:","block_group":"0986e41121a341e0a3a1208d14451d0c"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e31fe01792cf4677a427008725768e85","deepnote_cell_type":"text-cell-p"},"source":"\n1. Performance: XGBoost has a strong track record of producing high-quality results in various machine learning tasks, especially in Kaggle competitions, where it has been a popular choice for winning solutions.\n2. Scalability: XGBoost is designed for efficient and scalable training of machine learning models, making it suitable for large datasets.\n3. Customizability: XGBoost has a wide range of hyperparameters that can be adjusted to optimize performance, making it highly customizable.\n4. Handling of Missing Values: XGBoost has built-in support for handling missing values, making it easy to work with real-world data that often has missing values.\n5. Interpretability: Unlike some machine learning algorithms that can be difficult to interpret, XGBoost provides feature importances, allowing for a better understanding of which variables are most important in making predictions.","block_group":"8fa60e5d906d4503be61fd2ac4cd708e"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"78062d64c9784350a28a2e24e0a7f986","deepnote_cell_type":"text-cell-h3"},"source":"### Disadvantages of XGBoost:","block_group":"48cc5b22f0b847348eb2d54622d6c6b8"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a9b290732289411596b73072445bb50e","deepnote_cell_type":"text-cell-p"},"source":"1. Computational Complexity: XGBoost can be computationally intensive, especially when training large models, making it less suitable for resource-constrained systems.\n2. Overfitting: XGBoost can be prone to overfitting, especially when trained on small datasets or when too many trees are used in the model.\n3. Hyperparameter Tuning: XGBoost has many hyperparameters that can be adjusted, making it important to properly tune the parameters to optimize performance. However, finding the optimal set of parameters can be time-consuming and requires expertise.\n4. Memory Requirements: XGBoost can be memory-intensive, especially when working with large datasets, making it less suitable for systems with limited memory resources.\nThe two main factors to choose XGBoost over other algorithms are:","block_group":"51ca38e68abd495e9bb164953324156a"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f761a71b8bfd4705818979d6ada08840","deepnote_cell_type":"text-cell-h3"},"source":"### Reasons to Choose XGBoost","block_group":"bd36c2d32b4b45afa8b094429d3c8b55"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"4b061eadb63740e8967b4888072a65de","deepnote_cell_type":"text-cell-p"},"source":"1. Execution Speed\n2. Model Performance","block_group":"cbfffa283dc94c52bfc1e50ce9340979"},{"cell_type":"markdown","metadata":{"cell_id":"9c66eb6931314aa9855e29bc860f7a38","deepnote_cell_type":"markdown"},"source":"![](https://dzone.com/storage/temp/13069535-xgboost-features.png)","block_group":"e4e8bba2efb7427589f4e8e781858e35"},{"cell_type":"markdown","metadata":{"cell_id":"57fbfffbc1ee40ea89f595fb7a7c59f6","deepnote_cell_type":"markdown"},"source":"### Hyperparameter tuning","block_group":"c13faef6316f4e87950f55ff2b0e2f0c"},{"cell_type":"markdown","metadata":{"cell_id":"3e7f41bc61dc41deac2591ffe3e626a6","deepnote_cell_type":"markdown"},"source":"- n_estimators: The number of trees or rounds. Adding more trees will be at the risk of overfitting. The reason is in the way that the boosted tree model is constructed, sequentially where each new tree attempts to model and correct for the errors made by the sequence of previous trees. Quickly, the model reaches a point of diminishing returns.\n\n- max_depth: The maximum depth of a tree. It is also used to control overfitting as higher depth will allow model to learn relations very specific to a particular sample. Typically, it should be chosen from 3 to 10 and tuned using CV.\n\n- objective: The loss function to be minimized. binary:logistic is for binary classification, which will return predicted probability (NOT CLASS).\n\n- learning_rate: The convergence control parameter in gradient descent. It is intuitive that XGB will not reach its minimum if both n_estimaters and learning_rate are very small.\n\n- subsample: The fraction of observations to be randomly chosen for each tree. Lower values make the algorithm more conservative and prevents overfitting, but too small values might lead to underfitting. So, be careful to choose and the typical values are between 0.5 and 1.\n\n- min_child_weight: The minimum sum of weights all observations required in child. It is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\n\n- colsample_bytree: The fraction of features to use. By default it is set to 1 meaning that we will use all features. But in order to avoid the number of highly correlated trees is getting too big, we would like to use a sample of all the features for training to avoid overfitting.\n\n- scale_pos_weight: The parameter that controls the balance of positive and negative weights, useful for unbalanced classes. This dataset is unbalanced as we have seen, so we should be careful to tune it. The typical value to consider: sum(negative instances) / sum(positive instances).\n\n- gamma: The minimum loss reduction required to make a split. A node is split only when the resulting split gives a positive reduction in the loss function. The larger gamma is, the more conservative (overfitting) the algorithm will be. The values can vary depending on the loss function and should be tuned.\n\n- reg_alpha: L1 regularization term on weights. Increasing this value will make model more conservative.\n\n- reg_lambda: L2 regularization term on weights. Increasing this value will make model more conservative. Normalised to number of training examples.","block_group":"8518c3dcd701487a8c5bbe6e25dbd66a"},{"cell_type":"code","metadata":{"cell_id":"42d20bfd40e847fcb89d6ff87c4c5fe6","deepnote_cell_type":"code"},"source":"# params = {\n#         'min_child_weight': [1, 5, 10],\n#         'gamma': [0.5, 1, 1.5, 2, 5, 10],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5]\n#         }","block_group":"dc98328af1314944b51c0f38a62a6b4a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"bbd2fd5054fa477dad367ee463d6e756","deepnote_cell_type":"code"},"source":"# xgb = XGBClassifier(learning_rate=0.06, n_estimators=300, objective='binary:logistic',nthread=4)","block_group":"b54c8938ce78479c87c71670d964ce9e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"2acad3d4ce184fb1a730edd4c5081dbf","deepnote_cell_type":"code"},"source":"# from datetime import datetime\n# def timer(start_time=None):\n#     if not start_time:\n#         start_time = datetime.now()\n#         return start_time\n#     elif start_time:\n#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#         tmin, tsec = divmod(temp_sec, 60)\n#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n        \n# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# from sklearn.model_selection import StratifiedKFold\n\n\n# folds = 3\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\n# # Here we go\n# start_time = timer(None)\n# random_search.fit(X, y)\n# timer(start_time) ","block_group":"24ffc3248c504cd99cd43da037dfcc1e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"597841b1ad4b4892bb7e79ed92ea0fbb","deepnote_cell_type":"code"},"source":"# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n# print(random_search.best_score_ * 2 - 1)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)\n# results = pd.DataFrame(random_search.cv_results_)\n# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","block_group":"c9898a59cb89463491e9b5c3ffac9436","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"1a440be3483d426d97ee7500e05ac4d9","deepnote_cell_type":"markdown"},"source":"### Optimization","block_group":"713ae2764f0644b6bafcac94351627c9"},{"cell_type":"code","metadata":{"scrolled":true,"cell_id":"91a438840d194a7f96156a285ddefc7f","deepnote_cell_type":"code"},"source":"# 调参之后，较优的参数组合\n\nfrom xgboost import XGBClassifier\nMAX_ROUNDS = 400\nOPTIMIZE_ROUNDS = False\nLEARNING_RATE = 0.07\nEARLY_STOPPING_ROUNDS = 50  \n\nmodel = XGBClassifier(    \n                        n_estimators=MAX_ROUNDS,\n                        max_depth=4,\n                        objective=\"binary:logistic\",\n                        learning_rate=LEARNING_RATE, \n                        subsample=.8,\n                        min_child_weight=6,\n                        colsample_bytree=.8,\n                        scale_pos_weight=1.6,\n                        gamma=10,\n                        reg_alpha=8,\n                        reg_lambda=1.3,\n                     )","block_group":"7ce040f728b64d3c914ee42995a853c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"e3f460e8e4c74da8a0892aff0f51599d","deepnote_cell_type":"code"},"source":"def XGB_gini(df_train,tar_enc = True,pca = False):\n    \n    '''\n    df_train: 已处理的训练集数据\n    tar_enc: 是否对类别型变量使用target encoding\n    pca: 是否使用pca\n    '''    \n    \n    y = df_train.target\n    X = df_train.drop('target',axis=1)\n    \n    \n    y_valid_pred = 0*y\n    y_test_pred = 0\n    \n    \n    from target_encoding import target_encode\n    \n    train = pd.concat([X,y],axis=1)\n    for i, (train_index, test_index) in enumerate(kf.split(train)):\n\n        # 分成训练集、验证集、测试集\n\n        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()        \n        X_test = final_test.copy()\n        \n        \n        if pca == True:\n            n_comp = 20\n            print('\\nPCA执行中...')\n            pca = PCA(n_components=n_comp, svd_solver='full', random_state=1001)\n            X_train = pd.DataFrame(pca.fit_transform(X_train))\n            X_valid = pd.DataFrame(pca.transform(X_valid))\n            X_test = pd.DataFrame(pca.transform(final_test.copy()))\n        print( f\"\\n{i}折交叉验证： \")\n        \n        if pca == False:\n            if tar_enc == True:\n                f_cat = [f for f in X.columns if '_cat' in f and 'tar_enc' not in  f]\n                for f in f_cat:\n                    X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n                                                                    trn_series=X_train[f],\n                                                                    val_series=X_valid[f],\n                                                                    tst_series=X_test[f],\n                                                                    target=y_train,\n                                                                    min_samples_leaf=100,\n                                                                    smoothing=10,\n                                                                    noise_level=0\n                                                                    )\n\n    #     from category_encoders.target_encoder import TargetEncoder\n    #     tar_enc = TargetEncoder(cols = f_cat).fit(X_train,y_train)\n    #     X_train = tar_enc.transform(X_train) # 转换训练集\n    #     X_test = tar_enc.transform(X_test) # 转换测试集\n\n\n            X_train.drop(f_cat,axis=1,inplace=True)\n            X_valid.drop(f_cat,axis=1,inplace=True)\n            X_test.drop(f_cat,axis=1,inplace=True)\n\n\n        # 对于当前折，跑XGB\n        if OPTIMIZE_ROUNDS:\n            eval_set=[(X_valid,y_valid)]\n            fit_model = model.fit( X_train, y_train, \n                                   eval_set=eval_set,\n                                   eval_metric=gini_xgb,\n                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                                   verbose=False\n                                 )\n            print( \"  Best N trees = \", model.best_ntree_limit )\n            print( \"  Best gini = \", model.best_score )\n        else:\n            fit_model = model.fit( X_train, y_train )\n\n        # 生成验证集的预测结果\n        pred = fit_model.predict_proba(X_valid)[:,1]\n        print( \"  normalized gini coefficent = \", eval_gini(y_valid, pred) )\n        y_valid_pred.iloc[test_index] = pred\n\n        # 累积计算测试集预测结果\n        y_test_pred += fit_model.predict_proba(X_test)[:,1]\n\n        del X_test, X_train, X_valid, y_train\n\n    y_test_pred /= K  # 取各fold结果均值\n\n    print( \"\\n整个训练集（合并）的normalized gini coefficent:\" )\n    print( \"  final normalized gini coefficent = \", eval_gini(y, y_valid_pred) )\n    \n    return y_test_pred,eval_gini(y, y_valid_pred)","block_group":"284eb6f6229246259aa54e00aa1114bf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"64a9da346b53402fa14e15e0fa3a82a4","deepnote_cell_type":"code"},"source":"%%time\ny_test_pred, gini_score = XGB_gini(df_train=final_train,tar_enc=True)","block_group":"40526b4903704f259df0cfbf29c3344a","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n0折交叉验证： \n  normalized gini coefficent =  0.2538658383235931\n\n1折交叉验证： \n  normalized gini coefficent =  0.30728575577060424\n\n2折交叉验证： \n  normalized gini coefficent =  0.28508943739615844\n\n3折交叉验证： \n  normalized gini coefficent =  0.2829810430876034\n\n4折交叉验证： \n  normalized gini coefficent =  0.29113416245470913\n\n5折交叉验证： \n  normalized gini coefficent =  0.2940775662258974\n\n6折交叉验证： \n  normalized gini coefficent =  0.2818130027819927\n\n7折交叉验证： \n  normalized gini coefficent =  0.27666764206685135\n\n8折交叉验证： \n  normalized gini coefficent =  0.2723987713495525\n\n9折交叉验证： \n  normalized gini coefficent =  0.3070892136897676\n\n整个训练集（合并）的normalized gini coefficent:\n  final normalized gini coefficent =  0.28507826718380913\nWall time: 18min 55s\n"}]},{"cell_type":"code","metadata":{"cell_id":"662383a667e24490b2344d57f1b8e4ad","deepnote_cell_type":"code"},"source":"submission = pd.DataFrame()\nsubmission['id'] = final_test.index.values\nsubmission['target'] = y_test_pred\nsubmission.to_csv('xgb_submit.csv', float_format='%.6f', index=False)","block_group":"76eb2744a83d46e893e987dd95fa85da","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=185fc705-b61f-4bb6-be30-c3c88dd0b19b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"eaae645a02fb4565bb2a4e50fcbb0263","deepnote_execution_queue":[]}}
